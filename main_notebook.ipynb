{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a989f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='tqdm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "914e814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,glob,os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af92c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path=r\"C:\\Users\\thaku\\VS_code\\Langchain\\Fact_check\\data\\Fever\\train.jsonl\"\n",
    "val_path=r\"C:\\Users\\thaku\\VS_code\\Langchain\\Fact_check\\data\\Fever\\shared_task_dev.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb0beca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 145449 claims\n",
      "{\n",
      "  \"id\": 75397,\n",
      "  \"verifiable\": \"VERIFIABLE\",\n",
      "  \"label\": \"SUPPORTS\",\n",
      "  \"claim\": \"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\",\n",
      "  \"evidence\": [\n",
      "    [\n",
      "      [\n",
      "        92206,\n",
      "        104971,\n",
      "        \"Nikolaj_Coster-Waldau\",\n",
      "        7\n",
      "      ],\n",
      "      [\n",
      "        92206,\n",
      "        104971,\n",
      "        \"Fox_Broadcasting_Company\",\n",
      "        0\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def load_json(path):\n",
    "    with open(path,\"r\",encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "    \n",
    "train_data=load_json(train_path)\n",
    "print(f\"loaded {len(train_data)} claims\")\n",
    "\n",
    "print(json.dumps(train_data[0],indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "222eb571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'SUPPORTS': 80035, 'NOT ENOUGH INFO': 35639, 'REFUTES': 29775})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "labels=[d[\"label\"] for d in train_data]\n",
    "print(Counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea8b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features,Value,Sequence\n",
    "features=Features({\n",
    "    \"id\":Value(\"int64\"),\n",
    "    \"verifiable\":Value(\"string\"),\n",
    "    \"label\":Value(\"string\"),\n",
    "    \"claim\": Value(\"string\"),\n",
    "    \"evidence\":Sequence(Sequence(Sequence(Value(\"string\"))))\n",
    "})\n",
    "\n",
    "import ast\n",
    "\n",
    "for ex in train_data:\n",
    "    new_evidence = []\n",
    "    for ele in ex[\"evidence\"]:          # ele = list of evidence lines\n",
    "        converted_group = []\n",
    "        for group in ele:               # group = may already be a list OR a string like \"['92206', ...]\"\n",
    "            # if group is a string (stringified list), parse it safely\n",
    "            if isinstance(group, str):\n",
    "                try:\n",
    "                    group = ast.literal_eval(group)\n",
    "                except Exception:\n",
    "                    group = [group]\n",
    "            # ensure all elements are strings\n",
    "            converted_line = [str(item) for item in group]\n",
    "            converted_group.append(converted_line)\n",
    "        new_evidence.append(converted_group)\n",
    "    ex[\"evidence\"] = new_evidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df2def0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=load_json(val_path)\n",
    "\n",
    "for ex in val_data:\n",
    "    new_evidence = []\n",
    "    for ele in ex[\"evidence\"]:          # ele = list of evidence lines\n",
    "        converted_group = []\n",
    "        for group in ele:               # group = may already be a list OR a string like \"['92206', ...]\"\n",
    "            # if group is a string (stringified list), parse it safely\n",
    "            if isinstance(group, str):\n",
    "                try:\n",
    "                    group = ast.literal_eval(group)\n",
    "                except Exception:\n",
    "                    group = [group]\n",
    "            # ensure all elements are strings\n",
    "            converted_line = [str(item) for item in group]\n",
    "            converted_group.append(converted_line)\n",
    "        new_evidence.append(converted_group)\n",
    "    ex[\"evidence\"] = new_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e531b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds=Dataset.from_list(train_data,features=features)\n",
    "val_ds=Dataset.from_list(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc892d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['92206', '104971', 'Nikolaj_Coster-Waldau', '7'],\n",
       "  ['92206', '104971', 'Fox_Broadcasting_Company', '0']]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][\"evidence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "490891c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_path=r\"C:\\Users\\thaku\\VS_code\\Langchain\\Fact_check\\data\\Fever\\wiki-pages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "268686f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def load_fever_wiki(wiki_dir):\n",
    "    \"\"\"\n",
    "    Load all wiki-pages-XXX.jsonl files and build:\n",
    "      wiki_lookup[page_title][line_index] = sentence\n",
    "      wiki_text[page_title] = full text\n",
    "    \"\"\"\n",
    "    wiki_lookup = {}\n",
    "    wiki_text = {}\n",
    "    files = [f for f in os.listdir(wiki_dir) if f.endswith(\".jsonl\")]\n",
    "    for file in tqdm(sorted(files), desc=\"Loading Wikipedia pages\"):\n",
    "        with open(os.path.join(wiki_dir, file), \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    page = json.loads(line)\n",
    "                    page_id = page[\"id\"]\n",
    "                    page_lines = {}\n",
    "                    for line_item in page[\"lines\"].split(\"\\n\"):\n",
    "                        if not line_item.strip():\n",
    "                            continue\n",
    "                        parts = line_item.split(\"\\t\", 1)\n",
    "                        if len(parts) == 2:\n",
    "                            idx, sent = parts\n",
    "                            if sent.strip():\n",
    "                                page_lines[int(idx)] = sent.strip()\n",
    "                    if page_lines:\n",
    "                        wiki_lookup[page_id] = page_lines\n",
    "                    if page.get(\"text\"):\n",
    "                        wiki_text[page_id] = page[\"text\"]\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    print(f\"Loaded {len(wiki_lookup):,} Wikipedia pages.\")\n",
    "    return wiki_lookup, wiki_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9cfd428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia pages: 100%|██████████| 109/109 [04:15<00:00,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5,395,683 Wikipedia pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wiki_lookup, wiki_text=load_fever_wiki(wiki_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d677ff80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Cleaning wiki_lookup sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5395683 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5395683/5395683 [09:28<00:00, 9496.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Cleaning wiki_text pages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5395683/5395683 [09:55<00:00, 9056.74it/s] \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_wiki_text(text):\n",
    "    \"\"\"Clean a single sentence or text block from FEVER wiki dump artifacts.\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1️⃣ Remove tab-separated parts (entity links)\n",
    "    text = text.split(\"\\t\")[0]\n",
    "\n",
    "    # 2️⃣ Replace bracket placeholders\n",
    "    text = text.replace(\"-LRB-\", \"(\").replace(\"-RRB-\", \")\")\n",
    "\n",
    "    # 3️⃣ Replace double dashes with em-dash or space\n",
    "    text = text.replace(\"--\", \"–\")\n",
    "\n",
    "    # 4️⃣ Normalize spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_wiki_data(wiki_lookup, wiki_text):\n",
    "    \"\"\"Clean all sentences in wiki_lookup and full page text in wiki_text.\"\"\"\n",
    "    clean_lookup = {}\n",
    "    clean_text = {}\n",
    "\n",
    "    print(\"🧹 Cleaning wiki_lookup sentences...\")\n",
    "    for page, lines in tqdm(wiki_lookup.items()):\n",
    "        cleaned_lines = {}\n",
    "        for idx, sent in lines.items():\n",
    "            cleaned_lines[idx] = clean_wiki_text(sent)\n",
    "        clean_lookup[page] = cleaned_lines\n",
    "\n",
    "    print(\"🧹 Cleaning wiki_text pages...\")\n",
    "    for page, txt in tqdm(wiki_text.items()):\n",
    "        clean_text[page] = clean_wiki_text(txt)\n",
    "\n",
    "    return clean_lookup, clean_text\n",
    "wiki_lookup, wiki_text = clean_wiki_data(wiki_lookup, wiki_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01a0eee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Hunger Games is a 2012 American dystopian science fiction adventure film directed by Gary Ross and based on the novel of the same name by Suzanne Collins . It is the first installment in The Hunger Games film series and was produced by Nina Jacobson and Jon Kilik , with a screenplay by Ross , Collins , and Billy Ray . The film stars Jennifer Lawrence , Josh Hutcherson , Liam Hemsworth , Woody Harrelson , Elizabeth Banks , Lenny Kravitz , Stanley Tucci , and Donald Sutherland . The story takes place in a dystopian post-apocalyptic future in the nation of Panem , where boys and girls between the ages of 12 and 18 must take part in the Hunger Games , a televised annual event in which the `` tributes '' are required to fight to the death until there is only one survivor . Katniss Everdeen ( Lawrence ) volunteers to take her younger sister 's place . Joined by her district 's male tribute , Peeta Mellark ( Hutcherson ) , Katniss travels to the Capitol to train for the Hunger Games under the guidance of former victor Haymitch Abernathy ( Harrelson ) . Development of The Hunger Games began in March 2009 when Lions Gate Entertainment entered into a co-production agreement with Color Force , which had acquired the rights a few weeks earlier . Collins collaborated with Ray and Ross to write the screenplay . The screenplay expanded the character of Seneca Crane to allow several developments to be shown directly to the audience and Ross added several scenes between Crane and Coriolanus Snow . The main characters were cast between March and May 2011 . Principal photography began in May 2011 and ended in September 2011 , with filming taking place in North Carolina . The Hunger Games was shot entirely on film as opposed to digital . The film was released on March 21 , 2012 , in some European countries and in the US on March 23 , 2012 , in both conventional theaters and digital IMAX theaters . Japan received it last , on September 28 . When the film released , it set records for opening day ( $ 67.3 million ) and opening weekend for a non-sequel . At the time of its release , the film 's opening weekend gross ( $ 152.5 million ) was the third-largest of any movie in North America . It is the first film since Avatar to remain in first place at the North American box office for four consecutive weekends . The film was a massive box-office success by grossing over $ 694 million worldwide against its budget of $ 78 million , making it the third-highest-grossing film in the United States and ninth-highest-grossing worldwide of 2012 . It was released on DVD and Blu-ray Disc on August 18 , 2012 . With 7,434,058 units sold , the DVD was the best-selling DVD of 2012 . A sequel , The Hunger Games : Catching Fire , was released on November 22 , 2013 , in the United States . The Hunger Games received positive reviews from critics , with praise for its themes and messages , as well as Jennifer Lawrence 's portrayal of Katniss , though there was criticism of the film 's use of shaky cam in the action sequences . Like the novel , the film has been noted for its similarities to other works , such as Robert Sheckley 's short story `` Seventh Victim '' and its Italian film adaptation The 10th Victim , the Japanese novel Battle Royale and its film adaptation , and the Shirley Jackson short story `` The Lottery '' , with some criticizing The Hunger Games for being derivative of such works . Collins stated in an interview that her novel and screenplay drew on sources of inspiration such as the myth of Theseus , Roman gladiatorial games , reality television , and the desensitization of viewers to media coverage of real-life tragedy and war , not to think as just an audience member , `` Because those are real people on the screen , and they 're not going away when the commercials start to roll . '' The song `` Safe & Sound '' won a Grammy Award and was nominated for a Golden Globe Award for Best Original Song . For her performance , Lawrence won the Saturn Award for Best Actress , the Broadcast Film Critics Association Award for Best Actress in an Action Movie , the Empire Award for Best Actress and was also nominated for the New York Film Critics Circle Award for Best Actress .\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_text[\"The_Hunger_Games_-LRB-film-RRB-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19c2bc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'The Hunger Games is a 2012 American dystopian science fiction adventure film directed by Gary Ross and based on the novel of the same name by Suzanne Collins .',\n",
       " 1: 'It is the first installment in The Hunger Games film series and was produced by Nina Jacobson and Jon Kilik , with a screenplay by Ross , Collins , and Billy Ray .',\n",
       " 2: 'The film stars Jennifer Lawrence , Josh Hutcherson , Liam Hemsworth , Woody Harrelson , Elizabeth Banks , Lenny Kravitz , Stanley Tucci , and Donald Sutherland .',\n",
       " 3: \"The story takes place in a dystopian post-apocalyptic future in the nation of Panem , where boys and girls between the ages of 12 and 18 must take part in the Hunger Games , a televised annual event in which the `` tributes '' are required to fight to the death until there is only one survivor .\",\n",
       " 4: \"Katniss Everdeen ( Lawrence ) volunteers to take her younger sister 's place .\",\n",
       " 5: \"Joined by her district 's male tribute , Peeta Mellark ( Hutcherson ) , Katniss travels to the Capitol to train for the Hunger Games under the guidance of former victor Haymitch Abernathy ( Harrelson ) .\",\n",
       " 8: 'Development of The Hunger Games began in March 2009 when Lions Gate Entertainment entered into a co-production agreement with Color Force , which had acquired the rights a few weeks earlier .',\n",
       " 9: 'Collins collaborated with Ray and Ross to write the screenplay .',\n",
       " 10: 'The screenplay expanded the character of Seneca Crane to allow several developments to be shown directly to the audience and Ross added several scenes between Crane and Coriolanus Snow .',\n",
       " 11: 'The main characters were cast between March and May 2011 .',\n",
       " 12: 'Principal photography began in May 2011 and ended in September 2011 , with filming taking place in North Carolina .',\n",
       " 13: 'The Hunger Games was shot entirely on film as opposed to digital .',\n",
       " 16: 'The film was released on March 21 , 2012 , in some European countries and in the US on March 23 , 2012 , in both conventional theaters and digital IMAX theaters .',\n",
       " 17: 'Japan received it last , on September 28 .',\n",
       " 18: 'When the film released , it set records for opening day ( $ 67.3 million ) and opening weekend for a non-sequel .',\n",
       " 19: \"At the time of its release , the film 's opening weekend gross ( $ 152.5 million ) was the third-largest of any movie in North America .\",\n",
       " 20: 'It is the first film since Avatar to remain in first place at the North American box office for four consecutive weekends .',\n",
       " 21: 'The film was a massive box-office success by grossing over $ 694 million worldwide against its budget of $ 78 million , making it the third-highest-grossing film in the United States and ninth-highest-grossing worldwide of 2012 .',\n",
       " 22: 'It was released on DVD and Blu-ray Disc on August 18 , 2012 .',\n",
       " 23: 'With 7,434,058 units sold , the DVD was the best-selling DVD of 2012 .',\n",
       " 24: 'A sequel , The Hunger Games : Catching Fire , was released on November 22 , 2013 , in the United States .',\n",
       " 27: \"The Hunger Games received positive reviews from critics , with praise for its themes and messages , as well as Jennifer Lawrence 's portrayal of Katniss , though there was criticism of the film 's use of shaky cam in the action sequences .\",\n",
       " 28: \"Like the novel , the film has been noted for its similarities to other works , such as Robert Sheckley 's short story `` Seventh Victim '' and its Italian film adaptation The 10th Victim , the Japanese novel Battle Royale and its film adaptation , and the Shirley Jackson short story `` The Lottery '' , with some criticizing The Hunger Games for being derivative of such works .\",\n",
       " 29: \"Collins stated in an interview that her novel and screenplay drew on sources of inspiration such as the myth of Theseus , Roman gladiatorial games , reality television , and the desensitization of viewers to media coverage of real-life tragedy and war , not to think as just an audience member , `` Because those are real people on the screen , and they 're not going away when the commercials start to roll . ''\",\n",
       " 30: \"The song `` Safe & Sound '' won a Grammy Award and was nominated for a Golden Globe Award for Best Original Song .\",\n",
       " 31: 'For her performance , Lawrence won the Saturn Award for Best Actress , the Broadcast Film Critics Association Award for Best Actress in an Action Movie , the Empire Award for Best Actress and was also nominated for the New York Film Critics Circle Award for Best Actress .'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_lookup[\"The_Hunger_Games_-LRB-film-RRB-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c235ecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The_Hunger_Games_-LRB-film-RRB-\n",
      "The_Hunger_Games_-LRB-film-RRB-\n",
      "The_Hunger_Games_-LRB-film-RRB-\n",
      "The_Hunger_Games_-LRB-film-RRB-\n"
     ]
    }
   ],
   "source": [
    "for l in train_data[12][\"evidence\"]:\n",
    "    for group in l:\n",
    "        print(group[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de7045d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 198918,\n",
       " 'verifiable': 'VERIFIABLE',\n",
       " 'label': 'SUPPORTS',\n",
       " 'claim': 'International Relations includes communication.',\n",
       " 'evidence': [[['233882', '236772', 'International_relations', '9']]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[290]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23139386",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_data)):\n",
    "    # first extract topic name and correponding row number to look inside wiki lookup\n",
    "\n",
    "    topics=defaultdict(set)\n",
    "    for l in train_data[i][\"evidence\"]:\n",
    "        for group in l:\n",
    "            if group[2]!=None:\n",
    "                if group[3]!=None:\n",
    "                    topics[group[2]].add(int(group[3]))\n",
    "                else:\n",
    "                    topics[group[2]].add(-1)\n",
    "    evidence=[]\n",
    "    for topic in topics:\n",
    "        for line_number in topics[topic]:\n",
    "            evidence.append(wiki_lookup[topic][line_number])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aee9873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For example , international relations draws from the fields of : technology and engineering , economics , communication studies , history , international law , demography , philosophy , geography , social work , sociology , anthropology , criminology , psychology , gender studies , cultural studies , culturology , and diplomacy .']\n"
     ]
    }
   ],
   "source": [
    "topics=defaultdict(set)\n",
    "for l in train_data[290][\"evidence\"]:\n",
    "    for group in l:\n",
    "        if group[2]!=None:\n",
    "            if group[3]!=None:\n",
    "                topics[group[2]].add(int(group[3]))\n",
    "            else:\n",
    "                topics[group[2]].add(-1)\n",
    "evidence=[]\n",
    "for topic in topics:\n",
    "    for line_number in topics[topic]:\n",
    "        evidence.append(wiki_lookup[topic][line_number])\n",
    "print(evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b8d18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "def evaluate_sentence_page_alignment_no_norm(dataset, wiki_lookup):\n",
    "    \"\"\"\n",
    "    Evaluate FEVER dataset coverage using exact page_id matching (no normalization).\n",
    "    Handles int, str, or None sentence indices.\n",
    "    \"\"\"\n",
    "\n",
    "    stats = []\n",
    "    total = len(dataset)\n",
    "\n",
    "    for ex in tqdm(dataset, desc=\"Evaluating coverage (no normalization)\"):\n",
    "        has_page = False\n",
    "        has_sentence = False\n",
    "        has_page_no_sentence = False\n",
    "\n",
    "        for group in ex.get(\"evidence\", []):\n",
    "            for line in group:\n",
    "                if len(line) < 4:\n",
    "                    continue\n",
    "\n",
    "                page = line[2]       # exact FEVER page_id\n",
    "                sent_idx = line[3]   # may be int, str, or None\n",
    "\n",
    "                if not page:\n",
    "                    continue\n",
    "\n",
    "                # ✅ Page exists?\n",
    "                if page in wiki_lookup:\n",
    "                    has_page = True\n",
    "\n",
    "                    # ✅ Sentence index given?\n",
    "                    if sent_idx is None:\n",
    "                        # Page cited but no specific sentence reference\n",
    "                        has_page_no_sentence = True\n",
    "                        continue\n",
    "\n",
    "                    # ✅ Try to convert to integer safely\n",
    "                    try:\n",
    "                        sent_idx_int = int(sent_idx)\n",
    "                    except (ValueError, TypeError):\n",
    "                        sent_idx_int = None\n",
    "\n",
    "                    # ✅ Sentence index exists?\n",
    "                    if sent_idx_int is not None and sent_idx_int in wiki_lookup[page]:\n",
    "                        has_sentence = True\n",
    "                    else:\n",
    "                        has_page_no_sentence = True\n",
    "\n",
    "        stats.append({\n",
    "            \"id\": ex.get(\"id\"),\n",
    "            \"has_page\": has_page,\n",
    "            \"has_sentence\": has_sentence,\n",
    "            \"has_page_no_sentence\": has_page_no_sentence\n",
    "        })\n",
    "\n",
    "    # Aggregate results\n",
    "    c = Counter()\n",
    "    for s in stats:\n",
    "        if s[\"has_page\"]:\n",
    "            c[\"page\"] += 1\n",
    "        if s[\"has_sentence\"]:\n",
    "            c[\"sentence\"] += 1\n",
    "        if s[\"has_page_no_sentence\"]:\n",
    "            c[\"page_but_no_sentence\"] += 1\n",
    "        if not s[\"has_page\"]:\n",
    "            c[\"no_page\"] += 1\n",
    "\n",
    "    coverage_summary = {\n",
    "        \"total_examples\": total,\n",
    "        \"page_exists_%\": round(100 * c[\"page\"] / total, 2),\n",
    "        \"sentence_exists_%\": round(100 * c[\"sentence\"] / total, 2),\n",
    "        \"page_but_no_sentence_%\": round(100 * c[\"page_but_no_sentence\"] / total, 2),\n",
    "        \"no_page_%\": round(100 * c[\"no_page\"] / total, 2)\n",
    "    }\n",
    "\n",
    "    return coverage_summary, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c94be0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coverage (no normalization):   0%|          | 0/145449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coverage (no normalization): 100%|██████████| 145449/145449 [00:12<00:00, 11847.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Coverage Summary (handles None, str, int):\n",
      "total_examples                : 145449\n",
      "page_exists_%                 : 99.54\n",
      "sentence_exists_%             : 75.03\n",
      "page_but_no_sentence_%        : 24.5\n",
      "no_page_%                     : 0.46\n"
     ]
    }
   ],
   "source": [
    "coverage_summary, detailed_stats = evaluate_sentence_page_alignment_no_norm(train_data, wiki_lookup)\n",
    "\n",
    "print(\"\\n📊 Coverage Summary (handles None, str, int):\")\n",
    "for k, v in coverage_summary.items():\n",
    "    print(f\"{k:30s}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68be6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def extract_sentence_and_page_evidence(example, wiki_lookup, wiki_text):\n",
    "    \"\"\"\n",
    "    Extract both sentence-level and page-level evidence for one FEVER example.\n",
    "    Returns two strings: (sentence_evidence, page_evidence)\n",
    "    \"\"\"\n",
    "\n",
    "    topics = defaultdict(set)\n",
    "    for group in example.get(\"evidence\", []):\n",
    "        for line in group:\n",
    "            if len(line) >= 4:\n",
    "                page = line[2]\n",
    "                sent_idx = line[3]\n",
    "                if page is not None:\n",
    "                    if sent_idx is not None:\n",
    "                        try:\n",
    "                            topics[page].add(int(sent_idx))\n",
    "                        except ValueError:\n",
    "                            topics[page].add(-1)\n",
    "                    else:\n",
    "                        topics[page].add(-1)\n",
    "\n",
    "    sentence_evidence = []\n",
    "    page_evidence = []\n",
    "\n",
    "    for topic, line_nums in topics.items():\n",
    "        # sentence evidence: specific lines\n",
    "        if topic in wiki_lookup:\n",
    "            for line_number in line_nums:\n",
    "                if line_number >= 0 and line_number in wiki_lookup[topic]:\n",
    "                    sentence_evidence.append(wiki_lookup[topic][line_number])\n",
    "\n",
    "        # page evidence: entire text when -1 present\n",
    "        if topic in wiki_text:\n",
    "            page_evidence.append(wiki_text[topic])\n",
    "\n",
    "    sentence_evidence_text = \" \".join(sentence_evidence)\n",
    "    page_evidence_text = \" \".join(page_evidence)\n",
    "\n",
    "    return sentence_evidence_text, page_evidence_text\n",
    "\n",
    "\n",
    "for ex in train_data:\n",
    "    sent_evi, page_evi = extract_sentence_and_page_evidence(ex, wiki_lookup, wiki_text)\n",
    "    ex[\"sentence_evidence\"] = sent_evi\n",
    "    ex[\"page_evidence\"] = page_evi\n",
    "    # optional combined field\n",
    "    # ex[\"evidence_text\"] = f\"{sent_evi} {page_evi}\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3e43517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 76253,\n",
       " 'verifiable': 'VERIFIABLE',\n",
       " 'label': 'SUPPORTS',\n",
       " 'claim': 'There is a movie called The Hunger Games.',\n",
       " 'evidence': [[['93100', '106004', 'The_Hunger_Games_-LRB-film-RRB-', '0']],\n",
       "  [['93100', '106005', 'The_Hunger_Games_-LRB-film-RRB-', '1']],\n",
       "  [['93100', '106006', 'The_Hunger_Games_-LRB-film-RRB-', '2']],\n",
       "  [['93100', '106007', 'The_Hunger_Games_-LRB-film-RRB-', '16']]],\n",
       " 'sentence_evidence': 'The Hunger Games is a 2012 American dystopian science fiction adventure film directed by Gary Ross and based on the novel of the same name by Suzanne Collins . It is the first installment in The Hunger Games film series and was produced by Nina Jacobson and Jon Kilik , with a screenplay by Ross , Collins , and Billy Ray . The film stars Jennifer Lawrence , Josh Hutcherson , Liam Hemsworth , Woody Harrelson , Elizabeth Banks , Lenny Kravitz , Stanley Tucci , and Donald Sutherland . The film was released on March 21 , 2012 , in some European countries and in the US on March 23 , 2012 , in both conventional theaters and digital IMAX theaters .',\n",
       " 'page_evidence': \"The Hunger Games is a 2012 American dystopian science fiction adventure film directed by Gary Ross and based on the novel of the same name by Suzanne Collins . It is the first installment in The Hunger Games film series and was produced by Nina Jacobson and Jon Kilik , with a screenplay by Ross , Collins , and Billy Ray . The film stars Jennifer Lawrence , Josh Hutcherson , Liam Hemsworth , Woody Harrelson , Elizabeth Banks , Lenny Kravitz , Stanley Tucci , and Donald Sutherland . The story takes place in a dystopian post-apocalyptic future in the nation of Panem , where boys and girls between the ages of 12 and 18 must take part in the Hunger Games , a televised annual event in which the `` tributes '' are required to fight to the death until there is only one survivor . Katniss Everdeen ( Lawrence ) volunteers to take her younger sister 's place . Joined by her district 's male tribute , Peeta Mellark ( Hutcherson ) , Katniss travels to the Capitol to train for the Hunger Games under the guidance of former victor Haymitch Abernathy ( Harrelson ) . Development of The Hunger Games began in March 2009 when Lions Gate Entertainment entered into a co-production agreement with Color Force , which had acquired the rights a few weeks earlier . Collins collaborated with Ray and Ross to write the screenplay . The screenplay expanded the character of Seneca Crane to allow several developments to be shown directly to the audience and Ross added several scenes between Crane and Coriolanus Snow . The main characters were cast between March and May 2011 . Principal photography began in May 2011 and ended in September 2011 , with filming taking place in North Carolina . The Hunger Games was shot entirely on film as opposed to digital . The film was released on March 21 , 2012 , in some European countries and in the US on March 23 , 2012 , in both conventional theaters and digital IMAX theaters . Japan received it last , on September 28 . When the film released , it set records for opening day ( $ 67.3 million ) and opening weekend for a non-sequel . At the time of its release , the film 's opening weekend gross ( $ 152.5 million ) was the third-largest of any movie in North America . It is the first film since Avatar to remain in first place at the North American box office for four consecutive weekends . The film was a massive box-office success by grossing over $ 694 million worldwide against its budget of $ 78 million , making it the third-highest-grossing film in the United States and ninth-highest-grossing worldwide of 2012 . It was released on DVD and Blu-ray Disc on August 18 , 2012 . With 7,434,058 units sold , the DVD was the best-selling DVD of 2012 . A sequel , The Hunger Games : Catching Fire , was released on November 22 , 2013 , in the United States . The Hunger Games received positive reviews from critics , with praise for its themes and messages , as well as Jennifer Lawrence 's portrayal of Katniss , though there was criticism of the film 's use of shaky cam in the action sequences . Like the novel , the film has been noted for its similarities to other works , such as Robert Sheckley 's short story `` Seventh Victim '' and its Italian film adaptation The 10th Victim , the Japanese novel Battle Royale and its film adaptation , and the Shirley Jackson short story `` The Lottery '' , with some criticizing The Hunger Games for being derivative of such works . Collins stated in an interview that her novel and screenplay drew on sources of inspiration such as the myth of Theseus , Roman gladiatorial games , reality television , and the desensitization of viewers to media coverage of real-life tragedy and war , not to think as just an audience member , `` Because those are real people on the screen , and they 're not going away when the commercials start to roll . '' The song `` Safe & Sound '' won a Grammy Award and was nominated for a Golden Globe Award for Best Original Song . For her performance , Lawrence won the Saturn Award for Best Actress , the Broadcast Film Critics Association Award for Best Actress in an Action Movie , the Empire Award for Best Actress and was also nominated for the New York Film Critics Circle Award for Best Actress .\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7094c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace(\"-LSB-\", \"[\")\n",
    "    text = text.replace(\"-RSB-\", \"]\")\n",
    "    text = text.replace(\"-LRB-\", \"(\")\n",
    "    text = text.replace(\"-RRB-\", \")\")\n",
    "    text = text.replace(\"--\", \"–\")          # replace double dash with en dash\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0eced48",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_rows = []\n",
    "\n",
    "for ex in train_data:\n",
    "    claim = ex.get(\"claim\", \"\")\n",
    "    label = ex.get(\"label\", \"\")\n",
    "    sentence_evidence = clean_text(ex.get(\"sentence_evidence\", \"\"))\n",
    "    page_evidence = clean_text(ex.get(\"page_evidence\", \"\"))\n",
    "\n",
    "    # Combined context for RAG retrieval\n",
    "    combined_evidence = f\"{sentence_evidence} {page_evidence}\".strip()\n",
    "\n",
    "    rag_rows.append({\n",
    "        \"id\": ex.get(\"id\"),\n",
    "        \"claim\": claim,\n",
    "        \"label\": label,\n",
    "        \"sentence_evidence\": sentence_evidence,\n",
    "        \"page_evidence\": page_evidence\n",
    "    })\n",
    "\n",
    "rag_df = pd.DataFrame(rag_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a4be3485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence_evidence</th>\n",
       "      <th>page_evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90525</th>\n",
       "      <td>161307</td>\n",
       "      <td>The Kingdom of Georgia only lasted thirty years.</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>It reached its Golden Age of political and economic strength during the reign of King David IV a...</td>\n",
       "      <td>The Kingdom of Georgia , also known as the Georgian Empire , was a medieval monarchy which emerg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88786</th>\n",
       "      <td>106588</td>\n",
       "      <td>Marshall McLuhan taught English at University of Toronto as a Professor of English.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td></td>\n",
       "      <td>None may refer to : Zero , the mathematical concept of the quantity `` none '' The empty set , t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83303</th>\n",
       "      <td>87157</td>\n",
       "      <td>Dead Man Down features a Swedish actress born in 1979 in a lead role.</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>The film stars Colin Farrell , Noomi Rapace , Dominic Cooper , and Terrence Howard , and was rel...</td>\n",
       "      <td>Dead Man Down is an 2013 American neo-noir crime thriller film written by J.H. Wyman and directe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99252</th>\n",
       "      <td>9529</td>\n",
       "      <td>Robert Duvall received the National Medal of Arts in June of 2005.</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>He received the National Medal of Arts in 2005 .</td>\n",
       "      <td>Robert Selden Duvall ( [ duːˈvɔːl ] born January 5 , 1931 ) is an American actor and filmmaker ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27811</th>\n",
       "      <td>156594</td>\n",
       "      <td>Source Code had Jake Gyllenhaal in it.</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>Gyllenhaal received further recognition for roles in Zodiac ( 2007 ) , Brothers ( 2009 ) , Princ...</td>\n",
       "      <td>Jacob Benjamin Gyllenhaal ( [ ˈdʒɪlənhɑːl ] ; born December 19 , 1980 ) is an American actor . A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  \\\n",
       "90525  161307   \n",
       "88786  106588   \n",
       "83303   87157   \n",
       "99252    9529   \n",
       "27811  156594   \n",
       "\n",
       "                                                                                     claim  \\\n",
       "90525                                     The Kingdom of Georgia only lasted thirty years.   \n",
       "88786  Marshall McLuhan taught English at University of Toronto as a Professor of English.   \n",
       "83303                Dead Man Down features a Swedish actress born in 1979 in a lead role.   \n",
       "99252                   Robert Duvall received the National Medal of Arts in June of 2005.   \n",
       "27811                                               Source Code had Jake Gyllenhaal in it.   \n",
       "\n",
       "                 label  \\\n",
       "90525          REFUTES   \n",
       "88786  NOT ENOUGH INFO   \n",
       "83303         SUPPORTS   \n",
       "99252         SUPPORTS   \n",
       "27811         SUPPORTS   \n",
       "\n",
       "                                                                                         sentence_evidence  \\\n",
       "90525  It reached its Golden Age of political and economic strength during the reign of King David IV a...   \n",
       "88786                                                                                                        \n",
       "83303  The film stars Colin Farrell , Noomi Rapace , Dominic Cooper , and Terrence Howard , and was rel...   \n",
       "99252                                                     He received the National Medal of Arts in 2005 .   \n",
       "27811  Gyllenhaal received further recognition for roles in Zodiac ( 2007 ) , Brothers ( 2009 ) , Princ...   \n",
       "\n",
       "                                                                                             page_evidence  \n",
       "90525  The Kingdom of Georgia , also known as the Georgian Empire , was a medieval monarchy which emerg...  \n",
       "88786  None may refer to : Zero , the mathematical concept of the quantity `` none '' The empty set , t...  \n",
       "83303  Dead Man Down is an 2013 American neo-noir crime thriller film written by J.H. Wyman and directe...  \n",
       "99252  Robert Selden Duvall ( [ duːˈvɔːl ] born January 5 , 1931 ) is an American actor and filmmaker ....  \n",
       "27811  Jacob Benjamin Gyllenhaal ( [ ˈdʒɪlənhɑːl ] ; born December 19 , 1980 ) is an American actor . A...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "rag_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fc0a7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all rows: 145449it [02:26, 995.11it/s] \n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                          chunk_overlap=100,\n",
    "                                          length_function=len)\n",
    "\n",
    "docs=[]\n",
    "for i,row in tqdm(rag_df.iterrows(),desc=\"Loading all rows\"):\n",
    "    for chunk in splitter.split_text(row[\"page_evidence\"]):\n",
    "        docs.append({\n",
    "            \"id\":row[\"id\"],\n",
    "            \"claim\":row[\"claim\"],\n",
    "            \"label\":row[\"label\"],\n",
    "            \"chunk\":chunk\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a4e0f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing Langchain Documents: 100%|██████████| 651929/651929 [00:29<00:00, 21944.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from tqdm import tqdm\n",
    "\n",
    "lc_docs = [\n",
    "    Document(page_content=d[\"chunk\"],metadata={\"id\":d[\"id\"],\"claim\":d[\"claim\"],\"label\":d[\"label\"]})\n",
    "    for d in tqdm(docs,desc=\"Preparing Langchain Documents\")\n",
    "]\n",
    "\n",
    "embedding_fn=HuggingFaceEmbeddings(model_name=\"multi-qa-MiniLM-L6-cos-V1\")\n",
    "\n",
    "vector_store=FAISS.from_documents(lc_docs,embedding_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "633a59e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS vectorstore created and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "vector_store.save_local(\"./fever_faiss_store\")\n",
    "print(\"✅ FAISS vectorstore created and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4485e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"./fever_faiss_store\",\n",
    "    embedding_fn,\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0ebbd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU Name: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "952729b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",  # or gemini-1.5-pro for better reasoning\n",
    "    temperature=0.3,\n",
    "    max_output_tokens=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e4ed9cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Gemini Answer:\n",
      " SUPPORTS\n",
      "\n",
      "The evidence repeatedly states that \"Source Code is a 2011 American-French science fiction thriller film directed by Duncan Jones... It stars Jake Gyllenhaal as a U.S. Army captain...\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "# Output parser (returns a string)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Define prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a fact-checking assistant.\n",
    "Use the provided evidence to verify the claim and respond with one of:\n",
    "- SUPPORTS\n",
    "- REFUTES\n",
    "- NOT ENOUGH INFO\n",
    "\n",
    "Claim: {question}\n",
    "\n",
    "Evidence:\n",
    "{context}\n",
    "\n",
    "Your answer and brief reasoning:\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "# ✅ retrieve_context should only use retriever.invoke, no retriever pipe\n",
    "def retrieve_context(question: str) -> str:\n",
    "    docs = retriever.invoke(question)\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "# Parallel branch: question passthrough + retrieved context\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": RunnableLambda(retrieve_context)\n",
    "})\n",
    "\n",
    "# Main RAG chain\n",
    "main_chain = parallel_chain | prompt | llm | parser\n",
    "\n",
    "# Query\n",
    "query = \"Source Code had Jake Gyllenhaal in it.\t?\"\n",
    "\n",
    "# Invoke\n",
    "result = main_chain.invoke(query)\n",
    "\n",
    "print(\"🧠 Gemini Answer:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573e789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
